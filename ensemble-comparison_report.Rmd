---
title: "Ensemble Comparison Project"
author: "Li Shandross"
date: "`r Sys.Date()`" 
output: 
  pdf_document: 
    fig_width: 8
    fig_height: 10.5
indent: true
fontsize: 12pt
header-includes:
    - \usepackage{setspace}
    - \usepackage{indentfirst}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(zoltr)
library(covidHubUtils)
library(hubUtils)
library(hubEnsembles)
library(lubridate)
library(readr)
library(stringr)
library(tidyr)
library(patchwork)
library(knitr)

source("R/evaluation_functions.R")
```

# Introduction/Background
## Overview:
*Goal:* given quantile forecasts as an input, compare:
- linear pools
- direct computation of quantile average or median

### Related papers:
- By Emily: https://royalsocietypublishing.org/doi/10.1098/rsif.2022.0659
- Busetti: https://onlinelibrary.wiley.com/doi/full/10.1111/obes.12163?casa_token=J9KIeOZWEUAAAAAA%3A8Xm3PyoD9TqAczQf1P6C9QOEfHA-KePb06cLGtlEdM-MhFpbrXIHmD1bdP9tO0gcGiwh4mjPlcXtNVA

### Our setting:
- Primary interest is flu hospital admissions: will FluSight use the linear pool method this fall?
- But we only have 2 seasons of quantile forecasts for flu, and in the first season the data was iffy (full reporting only started halfway through the season). Therefore, if possible let’s also include COVID hospital admissions


# Methods
### Start with flu:
- Both seasons
- All locations forecasted in the flusight exercise. Anticipating that there might be a difference, let’s formally collect somewhere the locations used in flu so that we can easily use the same ones for covid if/when we add it in later.
- Include all forecast horizons, we can stratify results by horizon afterward
- All component models, no checks like whether the model provides all locations or all horizons. But the model should provide all quantile levels for any location/horizons where it provides any forecasts.

### Ensemble methods, all equally weighted:
- Linear pool, using `hubEnsembles::linear_pool`
  - *Note:* we’re calling distfromq to get from quantiles to full distributions. It makes some tail assumptions. The default is normal distributions for the tails. This is awkward because hospitalizations have to be non-negative, but a normal lower tail could extend into negative values. To address this, with this method let’s truncate any negative quantile estimates to 0.
  - ELR is working on adding in a lognormal tail assumption to distfromq. When it is ready, we can add it into the comparison (so there would be 2 LP methods, LP-normal and LP-lognormal)
- Mean of quantiles (Vincent-mean), using `hubEnsembles::simple_ensemble`
- Median of quantiles (Vincent-median), using `hubEnsembles::simple_ensemble`

## Data 
Flu incident hospitalization truth and forecasts are queried from zoltar for the 2021-2022 and 2022-23 seasons. This is the only target/time period evaluated currently.

```{r read in truth}
flu_dates_21_22 <- as.Date("2022-01-24") + weeks(0:21)
flu_dates_22_23 <- as.Date("2022-10-17") + weeks(0:30)
all_flu_dates <- c(flu_dates_21_22, flu_dates_22_23)
hub_locations_flusight <- tibble(hub_locations_flusight)

# Read in truth
flu_truth_all <- readr::read_rds("data/flu_truth_all.rds")
flu_truth_21_22 <- flu_truth_all |>
  dplyr::filter(target_end_date < "2022-08-01")
flu_truth_22_23 <- flu_truth_all |>
  dplyr::filter(target_end_date > "2022-08-01")
```

```{r plot truth}
flu_truth_plotting <- covidData::load_data(
  spatial_resolution = c("national", "state"),
  temporal_resolution = "weekly",
  measure = "flu hospitalizations",
  drop_last_date = FALSE
) %>%
  dplyr::left_join(covidData::fips_codes, by = "location") %>%
  dplyr::transmute(
    model="flu-truth",
    target_variable="inc flu hosp",
    target_end_date=date,
    location,
#    location_name = ifelse(location_name == "United States", "US", location_name),
    value = inc) #%>%

#load_truth("HealthData", "inc flu hosp", truth_end_date=as.Date("2023-08-01"), temporal_resolution="weekly", data_location = "covidData", hub="FluSight")

flu_truth_plotting %>%
  filter(
    location == "US", 
    target_end_date >= as.Date("2021-11-01"),
    target_end_date <= as.Date("2023-06-02")
  ) %>%
ggplot(aes(x = target_end_date, y = value)) + 
  geom_line() + 
  geom_point() + 
  geom_vline(xintercept = as.Date("2022-01-24"), linetype="dashed") +
  geom_vline(xintercept = as.Date("2022-06-20"), linetype="solid") +
  geom_vline(xintercept = as.Date("2022-10-17"), linetype="solid") +
  geom_vline(xintercept = as.Date("2023-05-15"), linetype="solid") +
  annotate(geom = "text", x = as.Date("2022-01-18"), y = 3000, 
           label = "Forecasting Starts", size = 5, angle = 90) + 
  annotate(geom = "text", x = as.Date("2022-06-14"), y = 2100, 
           label = "Season Ends", size = 5, angle = 90) + 
  annotate(geom = "text", x = as.Date("2022-10-11"), y = 2100, 
           label = "Season Starts", size = 5, angle = 90) + 
  annotate(geom = "text", x = as.Date("2023-05-20"), y = 2100, 
           label = "Season Ends", size = 5, angle = 90) + 
  annotate(geom = "text", x = as.Date("2022-03-02"), y = 25000, 
           label = "2021-22 Season", size = 8, angle = 0) + 
  annotate(geom = "text", x = as.Date("2023-02-05"), y = 25000, 
           label = "2022-23 Season", size = 8, angle = 0) + 
  scale_x_date(name=NULL, date_breaks = "4 month", minor_breaks = "2 month",
               date_labels = "%b 20%y") + 
  ylim(c(0, NA)) +
  theme(axis.ticks.length.x = unit(0.75, "cm"),  
        axis.text.x = element_text(vjust = 1, hjust = 0.5),
        legend.position = "none") + 
  theme_bw() +
  labs(title = "Influenza US National Hospitalizations",
       x = "Date", y = "Incident Influenza Hospitalizations")
```


## Forecast locations 
Forecasts are made for all 50 states, Washington DC, Puerto Rico, the Virgin Islands come and the US as a whole.
1 - 4 week ahead incident flu hospitalizations
Only validation phase, no testing phase (assume that will be the upcoming season).


##  Model specifications


## Metrics and evaluation  
- different ways to stratify evaluations: overall, by season, by horizon, by location, by (forecast) date
- separate by geographic scale (averaged states/territories vs US national) to avoid the US national results obscuring that of the state/territories
- Incident flu hospitalizations evaluated by usual horizon, incident covid hospitalizations will also be evaluated on a weekly basis
- metrics: average wis, average mae, average 50% pi coverage, average 95% pi coverage, average rwis, average rmae


# Results

```{r read in forecasts and scores}
flu_files <- list.files(path="data", pattern="ensemble_", full.names=TRUE)
flu_forecasts_ensembles <- purrr::map_dfr(flu_files, .f=read_rds) |>
    dplyr::select(model, forecast_date, location, horizon, temporal_resolution, 
                  target_variable, target_end_date, type, quantile, value)

flu_scores_baseline <- readr::read_rds("data/flu_baseline_scores.rds")
flu_scores_ensembles <- readr::read_rds("data/flu_scores_ensembles.rds")
flu_scores_all <- rbind(flu_scores_ensembles, flu_scores_baseline)

model_names <- c("Flusight-baseline", "mean-ensemble", "median-ensemble", "lp-normal", "lp-lognormal")
model_colors <- c("black", "#F8766D", "#B79F00", "#00BFC4", "#C77CFF")
```

## Plot Forecasts
Will plot forecasts for all models, select locations (including US, highest count state, lowest count state/territory)

## Overall model performance

```{r overall evaluation, message=FALSE, warning=FALSE}
flu_overall_us <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=NULL, baseline_name="Flusight-baseline", us_only=TRUE)
  
flu_overall_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=NULL, baseline_name="Flusight-baseline", us_only=FALSE)

knitr::kable(flu_overall_us, caption="Summary of overall model performance across both seasons, averaged over the US national geographic scale.")
knitr::kable(flu_overall_states, caption="Summary of overall model performance across both seasons, averaged over the states geographic scale.")
```

## By season model performance

```{r by season evaluation, message=FALSE, warning=FALSE}
flu_season_us <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables="season", baseline_name="Flusight-baseline", us_only=TRUE)

flu_season_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables="season", baseline_name="Flusight-baseline", us_only=FALSE)

knitr::kable(flu_season_us, caption="Summary of by season model performance, averaged over the US national geographic scale.")
knitr::kable(flu_season_states, caption="Summary of by season model performance, averaged over the states geographic scale.")
```


## Model performance by horizon

```{r by horizon evaluation, message=FALSE, warning=FALSE, fig.cap='Average WIS by the horizon for each model for the US national level and the states level.'}
flu_horizon_us <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables="horizon", baseline_name="Flusight-baseline", us_only=TRUE)

flu_horizon_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables="horizon", baseline_name="Flusight-baseline", us_only=FALSE)

# knitr::kable(flu_horizon_us, caption="Summary of by horizon model performance across both seasons, averaged over the US national geographic scale.")
# knitr::kable(flu_horizon_states, caption="Summary of by horizon model performance across both seasons, averaged over the states geographic scale.")

wis_plot_us <- plot_evaluated_scores(flu_horizon_us, model_names, model_colors, main="US")
wis_plot_states <- plot_evaluated_scores(flu_horizon_states, model_names, model_colors, main="States")

wis_plot_us + wis_plot_states +
  plot_layout(ncol = 2, guides='collect') &
  theme(legend.position='bottom')
```

### Additional by season split
```{r, message=FALSE, warning=FALSE}
flu_season_horizon_us <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "season"), baseline_name="Flusight-baseline", us_only=TRUE)

flu_season_horizon_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

wis_plot_us_2122 <- flu_season_horizon_us |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores(model_names, model_colors, main="US 2021-22")
wis_plot_us_2223 <- flu_season_horizon_us |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores(model_names, model_colors, main="US 2022-23")
wis_plot_states_2122 <- flu_season_horizon_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores(model_names, model_colors, main="States 2021-22")
wis_plot_states_2223 <- flu_season_horizon_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores(model_names, model_colors, main="States 2022-23")

wis_plot_us_2122 + wis_plot_us_2223 + wis_plot_states_2122+wis_plot_states_2223 +
  plot_layout(ncol = 2, guides='collect') &
  theme(legend.position='bottom')
```


## Model Performance by Week

```{r By date US 2021-22, message=FALSE, warning=FALSE, fig.cap='Average h-week ahead WIS and 95% PI coverage for each model for the US national level.'}  
flu_date_horizon_season_us <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=TRUE)

wis_date_plot_us1_2122 <- flu_date_horizon_season_us |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="US 2021-22, 1 week ahead")
wis_date_plot_us4_2122 <- flu_date_horizon_season_us |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="US 2021-22, 4 week ahead")
  
wis_date_plot_us1_2223 <- flu_date_horizon_season_us |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="US 2022-23, 1 week ahead")
wis_date_plot_us4_2223 <- flu_date_horizon_season_us |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="US 2022-23, 4 week ahead")
  
wis_date_plot_us1_2122 + wis_date_plot_us1_2223 + 
  wis_date_plot_us4_2122 + wis_date_plot_us4_2223 + 
  plot_layout(ncol = 2, guides='collect') &
  theme(legend.position='bottom')
```


```{r By date States 2021-22, message=FALSE, warning=FALSE, fig.cap='Average h-week ahead WIS and 95% PI coverage for each model for the States national level.'}  
flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

wis_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="States, 1 week ahead")
wis_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="States, 4 week ahead")
  
wis_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="States, 1 week ahead")
wis_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="States, 4 week ahead")
  
wis_date_plot_states1_2122 + wis_date_plot_states1_2223 + 
  wis_date_plot_states4_2122 + wis_date_plot_states4_2223 + 
  plot_layout(ncol = 2, guides='collect') &
  theme(legend.position='bottom')
```


## Model performance by location
```{r by location, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model across all horizons for both seasons'}  
flu_location <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables="location", baseline_name="Flusight-baseline")

model_levels <- pull(flu_overall_states, model)
wis_loc_plot <- plot_wis_loc(flu_scores_all, flu_truth_all, model_levels, baseline_name = "Flusight-baseline")
```


```{r by location and season 21 22, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model across all horizons during the 2021-2022 season'}  
wis_loc_plot_2122 <- flu_scores_all |>
  dplyr::mutate(season = ifelse(forecast_date < as.Date("2022-08-01"), "2021-2022", "2022-2023")) |>
  dplyr::filter(season == "2021-2022") |>
  plot_wis_loc(flu_truth_21_22, model_levels, baseline_name = "Flusight-baseline")
```
  
```{r by location and season 22 23, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model across all horizons during the 2022-2023 season'}  
wis_loc_plot_2223 <- flu_scores_all |>
  dplyr::mutate(season = ifelse(forecast_date < as.Date("2022-08-01"), "2021-2022", "2022-2023")) |>
  dplyr::filter(season == "2022-2023") |>
  plot_wis_loc(flu_truth_22_23, model_levels, baseline_name = "Flusight-baseline")
```


```{r by location and horizon 1, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model across both seasons for a 1 week ahead horizon'}  
model_levels_horizon1 <- filter(flu_horizon_states, horizon == 1)$model

wis_loc_plot1 <- flu_scores_all |>
  dplyr::filter(horizon == 1) |>
  plot_wis_loc(flu_truth_all, model_levels_horizon1, baseline_name = "Flusight-baseline")
```

```{r by location and horizon 4, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model across both seasons for a 4 week ahead horizon'}  
model_levels_horizon4 <- filter(flu_horizon_states, horizon == 4)$model

wis_loc_plot4 <- flu_scores_all |>
  dplyr::filter(horizon == 4) |>
  plot_wis_loc(flu_truth_all, model_levels_horizon4, baseline_name = "Flusight-baseline")
```


```{r by location horizon 1 and season 21 22, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model for a 1 week ahead horizon during the 2021-2022 season'}  
model_levels_horizon1_2122 <- filter(flu_season_horizon_states, season=="2021-2022", horizon == 1)$model

wis_loc_plot1_2122 <- flu_scores_all |>
  dplyr::mutate(season = ifelse(forecast_date < as.Date("2022-08-01"), "2021-2022", "2022-2023")) |>
  dplyr::filter(horizon==1, season == "2021-2022") |>
  plot_wis_loc(flu_truth_21_22, model_levels_horizon1_2122, baseline_name = "Flusight-baseline")
```

```{r by location horizon 4 and season 21 22, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model for a 4 week ahead horizon during the 2021-2022 season'}  
model_levels_horizon4_2122 <- filter(flu_season_horizon_states, season=="2021-2022", horizon == 4)$model

wis_loc_plot4_2122 <- flu_scores_all |>
  dplyr::mutate(season = ifelse(forecast_date < as.Date("2022-08-01"), "2021-2022", "2022-2023")) |>
  dplyr::filter(horizon==4, season == "2021-2022") |>
  plot_wis_loc(flu_truth_21_22, model_levels_horizon1_2122, baseline_name = "Flusight-baseline")
```

```{r by location horizon 1 and season 22 23, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model for a one week ahead horizon during the 2022-2023 season'}  
model_levels_horizon1_2223 <- filter(flu_season_horizon_states, season=="2022-2023", horizon == 1)$model

wis_loc_plot1_2223 <- flu_scores_all |>
  dplyr::mutate(season = ifelse(forecast_date < as.Date("2022-08-01"), "2021-2022", "2022-2023")) |>
  dplyr::filter(horizon==1, season == "2022-2023") |>
  plot_wis_loc(flu_truth_22_23, model_levels_horizon1_2223, baseline_name = "Flusight-baseline")
```
  
```{r by location horizon 4 and season 22 23, message=FALSE, warning=FALSE, fig.cap='Relative WIS plotted by location for each model for a one week ahead horizon during the 2022-2023 season'}  
model_levels_horizon4_2223 <- filter(flu_season_horizon_states, season=="2022-2023", horizon == 4)$model

wis_loc_plot4_2223 <- flu_scores_all |>
  dplyr::mutate(season = ifelse(forecast_date < as.Date("2022-08-01"), "2021-2022", "2022-2023")) |>
  dplyr::filter(horizon==4, season == "2022-2023") |>
  plot_wis_loc(flu_truth_22_23, model_levels_horizon4_2223, baseline_name = "Flusight-baseline")
```


# Conclusions/Discussion
Overall, the vincent median ensemble is the best of the ensembles. However, there are several instances where the two linear pools (and occasionally the vincent mean ensemble) beat the vincent median; namely, the 1-week ahead horizon for both geographic scales (though it's more pronounced for the averaged states level). A more granular breakdown reveals that this deviation from the overall trend occurs mostly during the 2022-2023 season, specifically during times of high change. These patterns in which the vincent mean ensemble generally performs best, except for the one week ahead horizon with combined seasons or just the 2022-2023 season when the linear pools perform slightly better.

There is only a marginal difference in performance between the two linear pools, even for the week-by-week break down of results. This suggests at least one of three scenarios: 1) the common quantile interpretation from monotonic splines plays a bigger role in defining the shape of the resulting probabilistic distribution the extrapolation for the tails, 2) there is not a meaningful difference between coercing negative values to zero for the linear pool with normal tails and using a linear pool with lognormal tails that are naturally non-negative, or 3) the original forecasts (which are non-negative) ensemble together in such a way that there is negligible difference between the two linear pools, but this may not always be the case for a different set of forecasts.

If the CDC is mainly interested in communicating forecasts with short-term horizons (e.g. 1-2 week ahead), a linear pool ensemble may be worth further investigation, especially during periods of rapid change. Either type of tail distribution would be acceptable, though normal tails with negative values coerced to zero may be preferred simply due to shorter computation time.


# References
\setlength{\parindent}{-0.4in}
\setlength{\leftskip}{0.4in}
\setlength{\parskip}{8pt}
\noindent

